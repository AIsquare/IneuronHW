{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Question 1.** Why we need to convert words into vector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Machine learning model take vecotrs(arays of numbers) as input. When working with text, the first thing we must do is come up with a strategy to convert strings to numbers before feeding it to the model.\n",
    "Well there are difference ways to convert strings to numbers\n",
    "the one like \"One-hot encoding\". Its better you look at the picture below to understand.\n",
    "![](one-hot.png)\n",
    "\n",
    "But it has its own disadvantage meaning there is no relation we can discuss about it. Matheamatically they are orthogonal to each other. We cannot describe the relation between the words which is not benefit to the NLP.\n",
    "\t\n",
    "There are different techniques like:\n",
    "1. One hot encoder\n",
    "2. Word Embeddings\n",
    "3. Word2vec etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** What are the differences in between CBOW (Continuous Bag-of-Words) and Skip-gram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW\n",
    "=====\n",
    "In CBOW we predict the target word from the context given below in the diagram.\n",
    "![](cbow.png)\n",
    "\n",
    "SKIP-GRAM\n",
    "==========\n",
    "And in Skip Gram we predict the context from the target word. Given below in the fig:\n",
    "\n",
    "\n",
    "![](skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Is CBOW and BOW (Bag-of-words) are the same?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words(BOW) is an approach for dealing with words and context in text processing or information retrieval. It refers to a way in which a group of words are represented without retaining order.\n",
    "Consider the below sentence.\n",
    "“I had a great time today.”\n",
    "The BOW representations of the sentence would be the collection of the six words without order. See the fig below\n",
    "![](bow.png)\n",
    "\n",
    "Whereas CBOW(continuous bag of words) as said CBOW is an architecture to train Word Embedding(vector representations of words, specifically Word2Vec) in whic we predict the traget word from the context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What are the use cases of CBOW and Skip-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general CBOW is used for small corpus it is faster to train. CBOW can be used to fill the next word like text suggestions.\n",
    "\n",
    "Whereas **skip gram** is slower and works on large corpus and large dimensions. \n",
    "\"I will have orange juice and eggs for breakfast.\"\n",
    "and a window size of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.**  Do you know any alternatives of Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe** is on the most popular alrternatives of.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Define Word Embedding in your own words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its another way(efficient) of learning words for computer.\n",
    "In short an embedding is a relatively low dimensional space in which you can translate high dimensional vectors.\n",
    "\n",
    "Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "Neural network embeddings have 3 primary purposes:\n",
    "Finding nearest neighbors in the embedding space. These can be used to make recommendations based on user interests or cluster categories.\n",
    "As input to a machine learning model for a supervised task.\n",
    "For visualization of concepts and relations between categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"800\"\n",
       "            src=\"https://projector.tensorflow.org/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b0d0c06e88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "url = 'https://projector.tensorflow.org/'\n",
    "\n",
    "IPython.display.IFrame(url, width=1300, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
