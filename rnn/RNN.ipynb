{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** What are deep learning modelling techniques to handle textual data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributional vectors, also known as word embeddings, are founded on the so-called distributional theory, which states that terms that occur in similar contexts have similar meanings. Word embeddings are pre-trained on a task that involves using a shallow neural network to predict a word based on its meaning. A neural language model proposed by bengio is depicted in the diagram below.\n",
    "![](rnn.png)\n",
    "Word vectors are responsible for SOTA in a variety of NLP tasks such as sentiment analysis and sentence compositionality because they appear to embed syntactical and semantic information.\n",
    "In the past, distributed representations were widely used to research various NLP activities, but it wasn't until the continuous bag-of-words (CBOW) and skip-gram models were introduced to the field that it began to gain traction. They were common because they could quickly build high-quality word embeddings and could be used for semantic compositionality (for example, ‘man' + ‘royal' = ‘king')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2vec**\n",
    "Word2vec: Both the CBOW and skip-gram models were proposed by Mikolav et al. in 2013. The aim of CBOW is to compute the conditional probability of a target word provided the background words in a given window size, and it is a neural approach to create word embeddings.Skip-gram, on the other hand, is a neural approach to word embedding construction, with the aim of predicting the surrounding background words (i.e., conditional probability) provided a central target word. The word embedding dimension is calculated in both models by computing (in an unsupervised manner) the prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is a neural-based technique in which a feature function is applied to constituent terms or n-grams in order to retrieve higher-level features. The abstract features that resulted have been successfully applied to tasks such as sentiment analysis, machine translation, and question answering, among others. Collobert and Weston were among the first to extend CNN-based framework systems to natural language processing.The aim of their method was to use a look-up table to convert words into vector representations, which resulted in a primitive word embedding approach that learned weights during the network's training:\n",
    "![](rnn1.png)\n",
    "To perform sentence modelling with a simple CNN, first tokenize sentences into terms, which are then transformed into a d-dimensional word embedding matrix (i.e., input embedding layer). Then, on this input embedding layer, convolutional filters are applied, which entails applying a filter with all possible window sizes to create a feature chart.\n",
    "The output is then max-pooled, which applies a max operation to each filter to produce a fixed length output and minimise the output's dimensionality. The result of this method is the final sentence representation.\n",
    "![](rnn2.png)\n",
    "Other NLP tasks such as NER, aspect detection, and POS can be studied by increasing the sophistication of the aforementioned basic CNN and adapting it to perform word-based predictions. This necessitates a window-based approach, in which each word is considered within a fixed-size window of neighbouring words (sub-sentence). After that, a standalone CNN is applied to the sub-sentence, with the training goal of predicting the word in the window's middle, also known as word-level classification.\n",
    "One of basic CNNs' flaws is their inability to model long-distance dependencies, which is critical for a variety of NLP tasks.\n",
    "Following that, a standalone CNN is applied to the sub-sentence, with the intention of training it to predict the middle word in the window, also known as word-level classification.\n",
    "The inability of simple CNNs to model long-distance dependencies, which is crucial for a variety of NLP tasks, is one of their shortcomings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** What is RNN and Why we use them?\n",
    "\n",
    "**Recurrent Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are advanced neural-based algorithms that are good at processing sequential data. An RNN performs a computation on each instance of an input sequence in a recursive manner, based on the previous computed results. These sequences are usually expressed by a fixed-size vector of tokens that are fed to a recurrent unit one by one.\n",
    "![](rnn3.png)\n",
    "The main strength of an RNN is the capacity to memorize the results of previous computations and use that information in the current computation. This makes RNN models suitable to model context dependencies in inputs of arbitrary length so as to create a proper composition of the input. RNNs have been used to study various NLP tasks such as machine translation, image captioning, and language modeling, among others.\n",
    "RNN Variants: An LSTM consist of three gates (input, forget, and output gates), and calculate the hidden state through a combination of the three. GRUs are similar to LSTMs but consist of only two gates and are more efficient because they are less complex. A study shows that it is difficult to say which of the gated RNNs are more effective, and they are usually picked based on the computing power available. Various LSTM-based models have been proposed for sequence to sequence mapping (via encoder-decoder frameworks) that are suitable for machine translation, text summarization, modeling human conversations, question answering, image-based language generation, among other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**.State down few problems in RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The computation of this neural network is slow, since the work is sequential \n",
    "therefor it takes O(t) to compute this makes problem harder instead solving it.\n",
    "> It's suffers from exploding gradient problem since it has to have memory of previous sequences\n",
    "in order to that it has to carry weights. If our weights is slifght bigger then computation makes it to bigger and bigger afterwhile. Eventually it has no significance.\n",
    "> The smake goes with the vanishing gradient problem if the parameters is slightly less such as 0.001 then after computation the number becomes near to 0 such that it has no effect in carrying any memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**.What is vanishing gradient and gradient explosion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**.What is LSTM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory Networks (LSTMs) are a form of RNN that can learn long-term dependencies. Hochreiter & Schmidhuber (1997) introduced them, and many people refined and popularised them in subsequent work. 1 They are now commonly used and perform exceptionally well on a wide range of problems.LSTMs was specifically designed to prevent the issue of long-term dependence. They don't have to work hard to remember details for long periods of time; it's basically second nature to them!\n",
    "\n",
    "Both recurrent neural networks are made up of a series of repeating neural network modules. This repeating module in standard RNNs will have a very simple structure, such as a single tanh layer.\n",
    "![](rnn4.png)\n",
    "LSTMs have a chain-like structure as well, but the repeating module is distinct. Instead of a single neural network layer, there are four, each of which interacts in a unique way.Don't be concerned with the specifics of what's going on. \n",
    "![](rnn5.png)\n",
    "The cell condition, the horizontal line running across the top of the diagram, is the key to LSTMs.\n",
    "The condition of the cell is similar to that of a conveyor belt. With just a few small linear interactions, it runs straight down the entire chain. It's really convenient for data to simply flow along it unaltered.The LSTM may remove or add information to the cell state, which is carefully controlled by structures called gates.\n",
    "Gates are a way to selectively allow information to pass through. A sigmoid neural net layer and a pointwise multiplication operation make them up.\n",
    "The first step is what information we are letting pass on.(sigmoid layer)\n",
    "The next step is to decide what new information we are going to store in the cell states.\n",
    "![](rnn6.png)\n",
    "We update the old state into the new cell state.\n",
    "Finally, we must determine what we can produce. This performance will be dependent on the state of our cells, but it will be filtered. First, we run a sigmoid layer to determine which aspects of the cell state will be output. The cell state is then passed through tanh (to force the values to be between 1 and 1) and multiplied by the output of the sigmoid gate, resulting in only the sections we want to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**.Benifits of LSTM over RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given time lags of uncertain length, LSTM is well-suited to identify, process, and forecast time series. LSTM has an advantage over other RNNs due to its relative insensitivity to gap length.\n",
    "The recurrent layer of all RNNs has feedback loops. This allows them to keep details in'memory' for a long time. However, training standard RNNs to solve problems that involve understanding long-term temporal dependencies can be difficult. This is due to the fact that the loss function's gradient decays exponentially over time (called the vanishing gradient problem).\n",
    "LSTM networks are a form of RNN that employs a combination of special and regular units. A'memory cell' is used in LSTM modules, which can store data for long periods of time. As information enters the memory, it is output, and it is forgotten, it is regulated by a series of gates. They will learn longer-term dependencies thanks to this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
