{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What are the differences in between NLTK and Stanfordnlp? What things we can easily do in NLTK but not in Stanfordnlp and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stnaford NLP has been changed to Stanza after version 1.0.0. Stanza is a python natural language analysis which is different from stanford core NLP which is built for Java.\n",
    "The Stanza comes with neural network components that also enable efficient training and evaluation with annotated data. The modules are built on top of the Pytorch libarary. It gives faster peroformance if you run this system on a GPU enabled machine.\n",
    "Stanza includes a python interface to CoreNLP Java package and inherits additonal functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.\n",
    "\n",
    "To summarize, Stanza features:\n",
    "\n",
    "Native Python implementation requiring minimal efforts to set up;\n",
    "Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition;\n",
    "Pretrained neural models supporting 66 (human) languages;\n",
    "A stable, officially maintained Python interface to CoreNLP.\n",
    "Below is an overview of Stanzaâ€™s neural network NLP pipeline:\n",
    "\n",
    "![](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NLTK is one of the oldest platform of building python program to work with huma language. It provides easy to use interface to over 50 corpora and lexical resources. Since the inception of different libraries NLTK still gives more flexibility as givning results as STRINGS wheras as others it gives as object in its own form which is more powerful.\n",
    "NLTK doesn't support word vector, which is a huge drawback in the times of AI.\n",
    "![](tree.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Which NLP library is best for dependency parsing?And Explain,Why that library is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every library is best in there own way. But Spacy stands out in explaining the Dependecy parsing in much more better way, \n",
    "they have visual representation of understanding the grammar which is more helpful in learning and implementing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Is Documents and sentence are same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any difference in documents and sentence because in NLP each word carries a weight.\n",
    "And we form vectors from these word to show the relations and based on tha we do the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What is the use of Multi-word tokens property in Stanfordnlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Word object holds a syntactic word and all of its word-level annotations. In the example of multi-word tokens (MWT), these are generated as a result of multi-word token expansion; Multi word token expansion is the process of splitting tokens into syntactic words which are used\n",
    "by downstream task such as part of speech tagging and dependency parsing.\n",
    "Each language has different rules for MWT expansion. \n",
    "For instance consider the Spanish sentence Pude haber querido escribirlo.. This sentence contains an example of an enclitic pronoun, which is split off from the verb by MWT expansion. So escribirlo is split into escribir and lo.\n",
    "\n",
    "Some MWT split decisions are made by a deterministic dictionary, while others are made by a statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
