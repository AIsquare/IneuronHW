{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What is Seq2seq?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence To Sequence, or Seq2Seq, is a sequence prediction model used in tasks like language modelling and machine translation. The idea is to read the input sequence one timestep at a time with one LSTM, the encoder, to obtain a large fixed dimensional vector representation (a background vector), and then extract the output sequence from that vector with another LSTM, the decoder.The second LSTM is similar to a recurrent neural network language model, but it is constrained by the input series.\n",
    "![](seq1.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** What is going inside the Encoder and Decoder?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stack of several recurrent units (for better results, LSTM or GRU cells) that each accepts a single element of the input sequence, collects information for that element, and propagates it forward.\n",
    "![](seq2.jpeg)\n",
    "The input sequence in a question-answering problem is a list of all words from the question. Each word is represented by the symbol x i, where I is the word's order.\n",
    "The hidden state:\n",
    "![](seq3.png)\n",
    "An ordinary recurrent neural network's output is represented by this simple formula. We simply add the necessary weights to the previous hidden state h (t-1) and the input vector x t, as you can see.\n",
    "Encoder Vector\n",
    "This is the model's final hidden state, created by the encoder. The formula above is used to measure it.\n",
    "This vector aims to encapsulate all input element information in order to aid the decoder in making accurate predictions.\n",
    "It serves as the model's initial hidden state for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder**<br>\n",
    "A set of recurrent units, each of which predicts an output y t at a given time phase t.\n",
    "Each recurrent unit takes in a hidden state from the previous unit and outputs as well as its own hidden state.\n",
    "The output sequence in the question-answering problem is a list of all words from the response. Each word is expressed as y i, where I denotes the word's order. Any hidden state h_i is computed using the formula:\n",
    "![](seq4.png)\n",
    "As you can see, we're simply computing the next hidden state by using the previous one.\n",
    "The formu is used to calculate the output y t at time stage t. We compute the outputs by combining the hidden state at the current time stage with the weight W. (S). Softmax is used to generate a probability vector that will aid in the prediction of the final result (e.g. word in the question-answering problem).\n",
    "This model's strength lies in its ability to map sequences of varying lengths to one another.\n",
    "![](seq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Attention mechanisms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow me to demonstrate how Attention functions in a translation task. Let's say we have the sentence \"How was your day?\" and we want to translate it into French as \"Comment se passe ta journ√©e.\" For each word in the output sentence, the Attention part of the network will map the important and significant words from the input sentence and assign higher weights to these words, improving the accuracy of the output prediction.\n",
    "![](seq6.jfif)\n",
    "To illustrate how the Attention Mechanism works, most papers will use sequence-to-sequence (seq2seq) models as an example. This is due to the fact that Attention was first brought as a way to solve the key problem with seq2seq models, and it was a huge success.\n",
    "Attention is simply a vector, which is often the product of a dense layer that employs the softmax function.\n",
    "Translation prior to the Attention mechanism relied on reading a full sentence and compressing all information into a fixed-length vector. As you can imagine, a sentence with hundreds of words expressed by many words would almost certainly result in information loss, poor translation, and so on.Attention, on the other hand, only partially solves the problem. It enables the machine translator to examine all of the information contained in the original sentence and then create the appropriate word based on the current word and context. It may also be possible for the interpreter to zoom in or out (focus on local or global features). The essence of the Probabilistic Language Model is to use Markov Assumption to assign a probability to a sentence. RNN is naturally implemented to model the conditional probability among words due to the existence of sentences that contain varying numbers of words. ![](seq7.png)\n",
    "Since translation often involves arbitrary input and output lengths, an encoder-decoder model is used, with the basic RNN cell replaced by a GRU or LSTM cell and hyperbolic tangent activation replaced by ReLU. Here, we're using a GRU cell.![](seq8.png)\n",
    "For computational efficiency, the embedding layer maps discrete words into dense vectors. The embedded word vectors are then sequentially fed into the encoder, also known as GRU cells. What occurred during the encoding process? Information flows from left to right, and each word vector is learned based on all previous words as well as the current input.At timestep 4, when the sentence has been fully read, the encoder produces an output and a secret state for further processing. For the encoding portion, the decoder (also known as GRUs) takes the encoder's hidden state, trains it using teacher forcing (a mode that uses the previous cell's output as current input), and then generates translation words sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
