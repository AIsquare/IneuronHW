{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What do you mean by transfer learning in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learnig is a means to extract knowledge from a source setting and apply it to a different target setting.\n",
    "![](transfer_learning_scenario.png)\n",
    "![](agenda.png)\n",
    "\n",
    "**A taxonomy that hightlights the variations can be seen below:**\n",
    "![](transfer_learning_taxonomy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.**Write name of 5 pretrained models famous in the field of NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT (Bidirectional Encoder Representations from Transformers) \n",
    "\n",
    "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "\n",
    "3. OpenAI’s GPT-3\n",
    "\n",
    "4. Baidu's Enhanced Representation through kNowledge IntEgration\n",
    "Baidu's ERNIE\n",
    "\n",
    "5. XLNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.**Diffrence betwwen auto encoding and auto regressive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive models are pretrained on the classic language modeling task: guess the next token having read all the previous ones. \n",
    "They correspond to the decoder of the original transformer model, and a mask is used on top of the full sentence so that the attention heads can only see what was before in the next, and not what’s after. Although those models can be fine-tuned and achieve great results on many tasks, the most natural application is text generation. A typical example of such models is GPT.\n",
    "These models rely on the decoder part of the original transformer and use an attention mask so that at each position, the model can only look at the tokens before in the attention heads.\n",
    "\n",
    "**whereas,**\n",
    "\n",
    "Autoencoding models are pretrained by corrupting the input tokens in some way and trying to reconstruct the original sentence. They correspond to the encoder of the original transformer model in the sense that they get access to the full inputs without any mask. Those models usually build a bidirectional representation of the whole sentence. They can be fine-tuned and achieve great results on many tasks such as text generation, but their most natural application is sentence classification or token classification. A typical example of such models is BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you mean by masked attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking is needed to prevent the attention mechanism of transformer cheating in the decoder \n",
    "when training( you may not find in encoder side).\n",
    "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is BERT used for ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT can be used for understanding the context of a language for example\n",
    "\"Joe biden is running for president\" means the same as \"Joe Biden is contesting\n",
    "president\".\n",
    "\n",
    "2. It is most widely used for text classification for ex: sentiment analysis.\n",
    "\n",
    "3. BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models.\n",
    "\n",
    "4. Since it's the product of GOOGLE it has been adopted for more adaptive search.\n",
    "\n",
    "5. Question Answering and ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is diffrence between GPT2 and BERT ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 consists of solely stacked decoder blocks from the transformer architecture.\n",
    "\n",
    "BERT, like GPT-2, uses the transformer architecture. However, it uses the **encoder** part instead of the decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
